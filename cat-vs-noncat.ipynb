{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here we start our logistic regression project \n",
      "\n",
      "Problem Statement: You are given a dataset (\"data.h5\") containing: - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px). You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat. \n",
      "\n",
      "loading the dataset. we run a method instead of including a module named load_dataset that is required in the imports in the course module. \n",
      "\n",
      "Each line of your train_set_x_orig and test_set_x_orig is an array representing an image \n",
      "\n",
      "the following is the image number 7 of the dataset. \n",
      "\n",
      "y = [1], it's a 'cat' picture. \n",
      "\n",
      "we loaded the following data: \n",
      "\n",
      "(209, 64, 64, 3)\n",
      "\n",
      "\n",
      "Exercise: Find the values for: - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0]. \n",
      "\n",
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n",
      "\n",
      "Exercise: Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px num_px 3, 1). \n",
      "\n",
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "train_set_x_flatten shape: (12288, 209)\n",
      "test_set_y shape: (1, 50)\n",
      "sanity check after reshaping: [17 71 49 38 70]\n",
      "\n",
      "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255. One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel). \n",
      "\n",
      "let's now determine how to create a neural network that help us solve the problem of identifying cat images\n",
      "\n",
      "the following is a set of steps we will use to solve the problem\n",
      "\n",
      "1. initialize the parameters of the model.\n",
      "\n",
      "2. learn the parameters for the model by minimizing the cost function.\n",
      "\n",
      "3. use the learned parameters to make predictions over the test set.\n",
      "\n",
      "4. analyse the results and conclude.\n",
      "\n",
      "implement sigmoid function to make predictions. use np.exp()\n",
      "we will start by defining the model structure and the helper methods to execute calculations base in our activation function, and the initialization step which requires an initialize function.\n",
      "\n",
      "1. define the model structure.\n",
      "\n",
      "sigmoid([0, 2]) = [0.5        0.88079708]\n",
      "\n",
      "Exercise: Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation.\n",
      "\n",
      "2. initialize the model parameter (attributes).\n",
      "\n",
      "w = [[0.]\n",
      " [0.]]\n",
      "\n",
      "b = 0\n",
      "\n",
      "3. calculate current loss, current gradient and update the parameters.\n",
      "\n",
      "Exercise: implement a function => propagate() that computes the cost function and its gradients.\n",
      "\n",
      "calculate the propagation function with weights = 1. and 2. , bias = 2. , data = [1.,2.,-1.],[3.,4.,-3.2] and  labels = [1, 0, 1]. \n",
      "\n",
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n",
      "Exercise: Implement the optimization function in the cell below.\n",
      "\n",
      "Write down the optimization function. The goal is to learn weights and bias by minimizing the cost function J. For a parameter theta, the update rule is theta = theta - activation * dtheta, where activation is the learning rate.\n",
      "\n",
      "weights = [[0.19033591]\n",
      " [0.12259159]]\n",
      "bias = 1.9253598300845747\n",
      "dw = [[0.67752042]\n",
      " [1.41625495]]\n",
      "db = 0.21919450454067657\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA71UlEQVR4nO3deVhV5f7//9cWdIMDoIQMhjgkDmRmWeT0zZSC8lhyyj6RppbDsSyHsoxTBmbJadBjdcqyHDIbLLNRy6HUo+KYaZI5oCBqjhggqaBw//7wxz5uQQQCNiyfj+tal+173Wut97r1Ovt11rrXXjZjjBEAAIBF1HB1AQAAAOWJcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAOgVJo0aaKBAwe6ugwAuCjCDeACs2bNks1m08aNG11dymXl5MmTio+P1/Lly11dipPp06erdevW8vDwUIsWLfTGG2+UeNucnByNHTtWQUFB8vT0VHh4uJYsWVJk38TERHXp0kW1a9dWQECARowYoezs7EL9fvrpJ0VFRcnLy0v16tXTbbfdps2bN5f19IBKR7gBUCo7duzQu+++6+oyyuTkyZMaP358lQo377zzjgYPHqywsDC98cYb6tixo0aMGKGXXnqpRNsPHDhQkydPVt++ffXaa6/Jzc1Nd9xxh1atWuXUb/PmzerRo4dOnjypyZMna/DgwZo2bZr69Onj1G/Tpk3q0qWL9uzZo7i4OD333HPatWuXbr75Zu3YsaPczhuoUAZApZs5c6aRZDZs2ODSOs6cOWNycnJcWsNfUdr6jx49aiSZuLi4iiuqFE6ePGl8fX1Nz549ndr79u1r6tSpY44fP17s9uvWrTOSzCuvvOJoO3XqlGnevLnp2LGjU9/bb7/dBAYGmszMTEfbu+++aySZRYsWOdruuOMOU79+fXPs2DFH2++//27q1q1r/v73v5fpPIHKxpUboAo7cOCAHnroIfn7+8tutyssLEwzZsxw6pObm6vnnntO119/vby9vVWnTh117dpVy5Ytc+qXmpoqm82mV199VVOmTFHz5s1lt9u1bds2xcfHy2azKTk5WQMHDpSPj4+8vb314IMP6uTJk077uXDOTcEtttWrV+vxxx+Xn5+f6tSpo+joaB09etRp2/z8fMXHxysoKEi1a9fWLbfcom3btpVoHk9x9ZdkDFJTU+Xn5ydJGj9+vGw2m2w2m+Lj4x19tm/frnvuuUcNGjSQh4eHOnTooK+//vpSf01ltmzZMqWnp+uRRx5xah8+fLj+/PNPLViwoNjt582bJzc3Nw0dOtTR5uHhoUGDBmnNmjXat2+fJCkrK0tLlixRv3795OXl5ejbv39/1a1bV59++qmjbeXKlYqIiJCvr6+jLTAwUDfffLO+/fbbIm9jAVWNu6sLAFC0w4cP66abbpLNZtOjjz4qPz8/fffddxo0aJCysrI0atQoSee+uN577z3FxMRoyJAhOnHihKZPn67IyEitX79e1157rdN+Z86cqdOnT2vo0KGy2+1q0KCBY929996rpk2bKiEhQZs2bdJ7772nhg0blugWyWOPPab69esrLi5OqampmjJlih599FHNnTvX0Sc2NlYvv/yyevXqpcjISG3ZskWRkZE6ffp0icelqPpLMgZ+fn6aOnWqHn74YUVHR+vvf/+7JOmaa66RJP3666/q3LmzGjVqpKefflp16tTRp59+qt69e+vzzz9XdHR0sXX98ccfysvLu2T9tWvXVu3atSVJP//8sySpQ4cOTn2uv/561ahRQz///LP69et30X39/PPPCg0NdQosknTjjTdKOncrKjg4WFu3btXZs2cLHadWrVq69tprHXVI5+bweHp6Fll3bm6ukpKSdNNNN13yPAGXcvWlI+ByVJLbUoMGDTKBgYFOtweMMea+++4z3t7e5uTJk8YYY86ePVvo1swff/xh/P39zUMPPeRoS0lJMZKMl5eXOXLkiFP/uLg4I8mpvzHGREdHG19fX6e2kJAQM2DAgELnEhERYfLz8x3to0ePNm5ubiYjI8MYY8yhQ4eMu7u76d27t9P+4uPjjSSnfRaluPpLOgbF3Zbq0aOHadu2rTl9+rSjLT8/33Tq1Mm0aNGi2NqMOTcuki65nH/s4cOHGzc3tyL35+fnZ+67775ijxkWFma6d+9eqP3XX381kszbb79tjDHms88+M5LMf//730J9+/TpYwICAhyf27Zta0JDQ83Zs2cdbTk5OaZx48ZGkpk3b16xNQFVAbelgCrIGKPPP/9cvXr1kjFGx44dcyyRkZHKzMzUpk2bJElubm6qVauWpHO3fY4fP+74f+kFfc539913O27PXGjYsGFOn7t27ar09HRlZWVdsuahQ4fKZrM5bZuXl6e9e/dKkn744QedPXu20C2Yxx577JL7vlT9pR2DCx0/flw//vij7r33Xp04ccIx1unp6YqMjNSuXbt04MCBYvfx4YcfasmSJZdc+vfv79jm1KlTjrov5OHhoVOnThV7zFOnTslutxe5bcH68/+8WN/zj/PII49o586dGjRokLZt26akpCT1799fBw8edNoXUJVxWwqogo4ePaqMjAxNmzZN06ZNK7LPkSNHHP/9/vvva9KkSdq+fbvOnDnjaG/atGmh7YpqK9C4cWOnz/Xr15d07pbLhbc+SrOtJEfIueqqq5z6NWjQwNG3JC5Wf2nG4ELJyckyxmjcuHEaN25ckX2OHDmiRo0aXXQfnTt3vuRxLuTp6anc3Nwi150+fbrI20MXbp+Tk1PktgXrz//zYn3PP86wYcO0b98+vfLKK3r//fclnbtt9tRTT+nFF19U3bp1S3BmgGsRboAqKD8/X5LUr18/DRgwoMg+BXNF5syZo4EDB6p379568skn1bBhQ7m5uSkhIUG7d+8utF1xX5hubm5FthtjLlnzX9m2NIqqv7RjcKGC8R4zZowiIyOL7HNhKLvQ0aNHSzTnpm7duo6AEBgYqLy8PB05ckQNGzZ09MnNzVV6erqCgoKK3VdgYGCRV5QKrrIUbB8YGOjUfmHfC4/z4osvasyYMfr111/l7e2ttm3b6p///KckKTQ09JLnCLga4Qaogvz8/FSvXj3l5eUpIiKi2L7z5s1Ts2bNNH/+fKfbQnFxcRVdZqmEhIRIOneV5PyrKenp6Y6rO2VV0jE4f935mjVrJkmqWbPmJcf7Ym644QbH1anixMXFOZ7QKpjsvXHjRt1xxx2OPhs3blR+fn6hyeAXuvbaa7Vs2TJlZWU5XVlbt26d0/6vvvpqubu7a+PGjbr33nsd/XJzc7V582antgL169dXly5dHJ+XLl2qK6+8Uq1atbrkOQKuxpwboApyc3PT3Xffrc8//1xJSUmF1p//iHXBFZPzr5CsW7dOa9asqfhCS6FHjx5yd3fX1KlTndr/85///OV9l3QMCp5SysjIcGpv2LChunXrpnfeeafIqxsXPtJelLLMuenevbsaNGhQaEymTp2q2rVrq2fPno62Y8eOafv27U6P5t9zzz3Ky8tzunWZk5OjmTNnKjw8XMHBwZIkb29vRUREaM6cOTpx4oSj7wcffKDs7OxCP+R3oblz52rDhg0aNWqUatTgawNVH1duABeaMWOGvv/++0LtI0eO1L/+9S8tW7ZM4eHhGjJkiNq0aaPjx49r06ZNWrp0qY4fPy5J+tvf/qb58+crOjpaPXv2VEpKit5++221adOmSv0mib+/v0aOHKlJkybpzjvvVFRUlLZs2aLvvvtOV1xxxUWvqpREScfA09NTbdq00dy5cxUaGqoGDRro6quv1tVXX60333xTXbp0Udu2bTVkyBA1a9ZMhw8f1po1a7R//35t2bKl2BrKOudmwoQJGj58uPr06aPIyEitXLlSc+bM0Ysvvuj0mP5//vMfjR8/XsuWLVO3bt0kSeHh4erTp49iY2N15MgRXXXVVXr//feVmpqq6dOnOx3rxRdfVKdOnXTzzTdr6NCh2r9/vyZNmqTbbrtNUVFRjn7//e9/9fzzz+u2226Tr6+v1q5dq5kzZyoqKkojR44s9TkCLuHCJ7WAy1bB49MXW/bt22eMMebw4cNm+PDhJjg42NSsWdMEBASYHj16mGnTpjn2lZ+fbyZOnGhCQkKM3W437du3N99++60ZMGCACQkJcfQreJT6/F+zLVDwKPjRo0eLrDMlJcXRdrFHwS98rH3ZsmVGklm2bJmj7ezZs2bcuHEmICDAeHp6mu7du5vffvvN+Pr6mmHDhhU7ZsXVX9IxMMaYxMREc/3115tatWoVejR79+7dpn///iYgIMDUrFnTNGrUyPztb3+r8Mefp02bZlq2bGlq1aplmjdvbv797387PVZvzP/+js4fT2PO/SLxmDFjTEBAgLHb7eaGG24w33//fZHHWblypenUqZPx8PAwfn5+Zvjw4SYrK8upT3JysrntttvMFVdcYex2u2nVqpVJSEio1r9kjcuPzZhynu0HAKWQkZGh+vXr64UXXtAzzzzj6nIAWAA3TwFUmqJ+I2XKlCmS5LjVAgB/FXNuAFSauXPnatasWbrjjjtUt25drVq1Sh9//LFuu+22Ms1ZAYCiEG4AVJprrrlG7u7uevnll5WVleWYZPzCCy+4ujQAFsKcGwAAYCnMuQEAAJZCuAEAAJZy2c25yc/P1++//6569er9pR8NAwAAlccYoxMnTigoKOiSv5R92YWb33//3fGT5AAAoHrZt2+frrzyymL7XHbhpl69epLODc75L5oDAABVV1ZWloKDgx3f48W57MJNwa0oLy8vwg0AANVMSaaUMKEYAABYCuEGAABYCuEGAABYCuEGAABYCuEGAABYisvDzYEDB9SvXz/5+vrK09NTbdu21caNGy/a/+DBg7r//vsVGhqqGjVqaNSoUZVXLAAAqPJcGm7++OMPde7cWTVr1tR3332nbdu2adKkSapfv/5Ft8nJyZGfn5+effZZtWvXrhKrBQAA1YFLf+fmpZdeUnBwsGbOnOloa9q0abHbNGnSRK+99pokacaMGRVaHwAAqH5ceuXm66+/VocOHdSnTx81bNhQ7du317vvvluux8jJyVFWVpbTAgAArMul4WbPnj2aOnWqWrRooUWLFunhhx/WiBEj9P7775fbMRISEuTt7e1YeK8UAADWZjPGGFcdvFatWurQoYMSExMdbSNGjNCGDRu0Zs2aS27frVs3XXvttZoyZcpF++Tk5CgnJ8fxueDdFJmZmbx+AQCAaiIrK0ve3t4l+v526ZWbwMBAtWnTxqmtdevWSktLK7dj2O12x3ukeJ8UAADW59Jw07lzZ+3YscOpbefOnQoJCXFRRQAAoLpz6dNSo0ePVqdOnTRx4kTde++9Wr9+vaZNm6Zp06Y5+sTGxurAgQOaPXu2o23z5s2SpOzsbB09elSbN29WrVq1Cl0FAgAAlx+XzrmRpG+//VaxsbHatWuXmjZtqscff1xDhgxxrB84cKBSU1O1fPlyR1tRrzsPCQlRamrqJY9Xmnt2AACgaijN97fLw01lI9wAAFD9VJsJxQAAAOWNcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACyFcAMAACzF5eHmwIED6tevn3x9feXp6am2bdtq48aNxW6zfPlyXXfddbLb7brqqqs0a9asyikWAABUeS4NN3/88Yc6d+6smjVr6rvvvtO2bds0adIk1a9f/6LbpKSkqGfPnrrlllu0efNmjRo1SoMHD9aiRYsqsXIAAFBV2YwxxlUHf/rpp7V69WqtXLmyxNuMHTtWCxYsUFJSkqPtvvvuU0ZGhr7//vtLbp+VlSVvb29lZmbKy8urTHUDAIDKVZrvb5deufn666/VoUMH9enTRw0bNlT79u317rvvFrvNmjVrFBER4dQWGRmpNWvWFNk/JydHWVlZTgsAALAul4abPXv2aOrUqWrRooUWLVqkhx9+WCNGjND7779/0W0OHTokf39/pzZ/f39lZWXp1KlThfonJCTI29vbsQQHB5f7eQAAgKrDpeEmPz9f1113nSZOnKj27dtr6NChGjJkiN5+++1yO0ZsbKwyMzMdy759+8pt3wAAoOpxabgJDAxUmzZtnNpat26ttLS0i24TEBCgw4cPO7UdPnxYXl5e8vT0LNTfbrfLy8vLaQEAANbl0nDTuXNn7dixw6lt586dCgkJueg2HTt21A8//ODUtmTJEnXs2LFCagQAANWLS8PN6NGjtXbtWk2cOFHJycn66KOPNG3aNA0fPtzRJzY2Vv3793d8HjZsmPbs2aOnnnpK27dv11tvvaVPP/1Uo0ePdsUpAACAKsal4eaGG27QF198oY8//lhXX321JkyYoClTpqhv376OPgcPHnS6TdW0aVMtWLBAS5YsUbt27TRp0iS99957ioyMdMUpAACAKsalv3PjCvzODQAA1U+1+Z0bAACA8ka4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAlkK4AQAAluLScBMfHy+bzea0tGrV6qL9z5w5o+eff17NmzeXh4eH2rVrp++//74SKwYAAFWdu6sLCAsL09KlSx2f3d0vXtKzzz6rOXPm6N1331WrVq20aNEiRUdHKzExUe3bt6+McgEAQBXn8nDj7u6ugICAEvX94IMP9Mwzz+iOO+6QJD388MNaunSpJk2apDlz5lRkmQAAoJpw+ZybXbt2KSgoSM2aNVPfvn2VlpZ20b45OTny8PBwavP09NSqVauK3SYrK8tpAQAA1uXScBMeHq5Zs2bp+++/19SpU5WSkqKuXbvqxIkTRfaPjIzU5MmTtWvXLuXn52vJkiWaP3++Dh48eNFjJCQkyNvb27EEBwdX1OkAAIAqwGaMMa4uokBGRoZCQkI0efJkDRo0qND6o0ePasiQIfrmm29ks9nUvHlzRUREaMaMGTp16lSR+8zJyVFOTo7jc1ZWloKDg5WZmSkvL68KOxcAAFB+srKy5O3tXaLvb5ffljqfj4+PQkNDlZycXOR6Pz8/ffnll/rzzz+1d+9ebd++XXXr1lWzZs0uuk+73S4vLy+nBQAAWFeVCjfZ2dnavXu3AgMDi+3n4eGhRo0a6ezZs/r888911113VVKFAACgqnNpuBkzZoxWrFih1NRUJSYmKjo6Wm5uboqJiZEk9e/fX7GxsY7+69at0/z587Vnzx6tXLlSUVFRys/P11NPPeWqUwAAAFWMSx8F379/v2JiYpSeni4/Pz916dJFa9eulZ+fnyQpLS1NNWr8L3+dPn1azz77rPbs2aO6devqjjvu0AcffCAfHx8XnQEAAKhqqtSE4spQmglJAACgaqi2E4oBAAD+KsINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwlDKFm9mzZysnJ6dQe25urmbPnv2XiwIAACgrmzHGlHYjNzc3HTx4UA0bNnRqT09PV8OGDZWXl1duBZa3rKwseXt7KzMzU15eXq4uBwAAlEBpvr/LdOXGGCObzVaoff/+/fL29i7LLgEAAMqFe2k6t2/fXjabTTabTT169JC7+/82z8vLU0pKiqKiosq9SAAAgJIqVbjp3bu3JGnz5s2KjIxU3bp1Hetq1aqlJk2a6O677y7XAgEAAEqjVOEmLi5OktSkSRPdd999stvtFVIUAABAWZVpzk337t119OhRx+f169dr1KhRmjZtWrkVBgAAUBZlCjf333+/li1bJkk6dOiQIiIitH79ej3zzDN6/vnny7VAAACA0ihTuElKStKNN94oSfr000/Vtm1bJSYm6sMPP9SsWbPKsz4AAIBSKVO4OXPmjGO+zdKlS3XnnXdKklq1aqWDBw+WeD/x8fGOp68KllatWhW7zZQpU9SyZUt5enoqODhYo0eP1unTp8tyGgAAwIJKNaG4QFhYmN5++2317NlTS5Ys0YQJEyRJv//+u3x9fUu9r6VLl/6vIPeLl/TRRx/p6aef1owZM9SpUyft3LlTAwcOlM1m0+TJk8tyKgAAwGLKFG5eeuklRUdH65VXXtGAAQPUrl07SdLXX3/tuF1V4gLc3RUQEFCivomJiercubPuv/9+Seee2oqJidG6detKdwIAAMCyyhRuunXrpmPHjikrK0v169d3tA8dOlS1a9cu1b527dqloKAgeXh4qGPHjkpISFDjxo2L7NupUyfNmTNH69ev14033qg9e/Zo4cKFeuCBBy66/5ycHKf3YGVlZZWqPgAAUL2UKdxI594vdfbsWa1atUqS1LJlSzVp0qRU+wgPD9esWbPUsmVLHTx4UOPHj1fXrl2VlJSkevXqFep///3369ixY+rSpYuMMTp79qyGDRumf/7znxc9RkJCgsaPH1+qugAAQPVVphdn/vnnn3rsscc0e/Zs5efnSzoXdvr376833nij1FdvCmRkZCgkJESTJ0/WoEGDCq1fvny57rvvPr3wwgsKDw9XcnKyRo4cqSFDhmjcuHFF7rOoKzfBwcG8OBMAgGqkwl+c+fjjj2vFihX65ptvlJGRoYyMDH311VdasWKFnnjiiTIVLUk+Pj4KDQ1VcnJykevHjRunBx54QIMHD1bbtm0VHR2tiRMnKiEhwRGyLmS32+Xl5eW0AAAA6ypTuPn88881ffp03X777Y7AcMcdd+jdd9/VvHnzylxMdna2du/ercDAwCLXnzx5UjVqOJfs5uYm6dybygEAAMoUbk6ePCl/f/9C7Q0bNtTJkydLvJ8xY8ZoxYoVSk1NVWJioqKjo+Xm5qaYmBhJUv/+/RUbG+vo36tXL02dOlWffPKJUlJStGTJEo0bN069evVyhBwAAHB5K9OE4o4dOyouLk6zZ8+Wh4eHJOnUqVMaP368OnbsWOL97N+/XzExMUpPT5efn5+6dOmitWvXys/PT5KUlpbmdKXm2Weflc1m07PPPqsDBw7Iz89PvXr10osvvliW0wAAABZUpgnFW7duVVRUlHJychy/cbNlyxbZ7XYtXrxYYWFh5V5oeSnNhCQAAFA1lOb7u0zhRjp3a+rDDz/U9u3bJUmtW7dW37595enpWZbdVRrCDQAA1U9pvr/LdFsqISFB/v7+GjJkiFP7jBkzdPToUY0dO7YsuwUAAPjLyjSh+J133inyBZcF75wCAABwlTKFm0OHDhX5uLafn1+p3goOAABQ3soUboKDg7V69epC7atXr1ZQUNBfLgoAAKCsyjTnZsiQIRo1apTOnDmj7t27S5J++OEHPfXUU3/pF4oBAAD+qjKFmyeffFLp6el65JFHlJubK0ny8PDQ2LFjnX50DwAAoLKV+VFw6dzrEn777Td5enqqRYsWstvt5VlbheBRcAAAqp8KfxS8QN26dXXDDTf8lV0AAACUqzJNKAYAAKiqCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSCDcAAMBSXBpu4uPjZbPZnJZWrVpdtH+3bt0K9bfZbOrZs2clVg0AAKoyd1cXEBYWpqVLlzo+u7tfvKT58+crNzfX8Tk9PV3t2rVTnz59KrRGAABQfbg83Li7uysgIKBEfRs0aOD0+ZNPPlHt2rUJNwAAwMHlc2527dqloKAgNWvWTH379lVaWlqJt50+fbruu+8+1alTpwIrBAAA1YlLr9yEh4dr1qxZatmypQ4ePKjx48era9euSkpKUr169Yrddv369UpKStL06dOL7ZeTk6OcnBzH56ysrHKpHQAAVE02Y4xxdREFMjIyFBISosmTJ2vQoEHF9v3HP/6hNWvW6Jdffim2X3x8vMaPH1+oPTMzU15eXn+pXgAAUDmysrLk7e1dou9vl9+WOp+Pj49CQ0OVnJxcbL8///xTn3zyySUDkCTFxsYqMzPTsezbt6+8ygUAAFVQlQo32dnZ2r17twIDA4vt99lnnyknJ0f9+vW75D7tdru8vLycFgAAYF0uDTdjxozRihUrlJqaqsTEREVHR8vNzU0xMTGSpP79+ys2NrbQdtOnT1fv3r3l6+tb2SUDAIAqzqUTivfv36+YmBilp6fLz89PXbp00dq1a+Xn5ydJSktLU40azvlrx44dWrVqlRYvXuyKkgEAQBVXpSYUV4bSTEgCAABVQ7WdUAwAAPBXEW4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAICluDTcxMfHy2azOS2tWrUqdpuMjAwNHz5cgYGBstvtCg0N1cKFCyupYgAAUNW5u7qAsLAwLV261PHZ3f3iJeXm5urWW29Vw4YNNW/ePDVq1Eh79+6Vj49PJVQKAACqA5eHG3d3dwUEBJSo74wZM3T8+HElJiaqZs2akqQmTZpUYHUAAKC6cfmcm127dikoKEjNmjVT3759lZaWdtG+X3/9tTp27Kjhw4fL399fV199tSZOnKi8vLxKrBgAAFRlLr1yEx4erlmzZqlly5Y6ePCgxo8fr65duyopKUn16tUr1H/Pnj368ccf1bdvXy1cuFDJycl65JFHdObMGcXFxRV5jJycHOXk5Dg+Z2VlVdj5AAAA17MZY4yriyiQkZGhkJAQTZ48WYMGDSq0PjQ0VKdPn1ZKSorc3NwkSZMnT9Yrr7yigwcPFrnP+Ph4jR8/vlB7ZmamvLy8yvcEAABAhcjKypK3t3eJvr9dflvqfD4+PgoNDVVycnKR6wMDAxUaGuoINpLUunVrHTp0SLm5uUVuExsbq8zMTMeyb9++CqkdAABUDVUq3GRnZ2v37t0KDAwscn3nzp2VnJys/Px8R9vOnTsVGBioWrVqFbmN3W6Xl5eX0wIAAKzLpeFmzJgxWrFihVJTU5WYmKjo6Gi5ubkpJiZGktS/f3/FxsY6+j/88MM6fvy4Ro4cqZ07d2rBggWaOHGihg8f7qpTAAAAVYxLJxTv379fMTExSk9Pl5+fn7p06aK1a9fKz89PkpSWlqYaNf6Xv4KDg7Vo0SKNHj1a11xzjRo1aqSRI0dq7NixrjoFAABQxVSpCcWVoTQTkgAAQNVQbScUAwAA/FWEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCmEGwAAYCkuDTfx8fGy2WxOS6tWrS7af9asWYX6e3h4VGLFAACgqnN3dQFhYWFaunSp47O7e/EleXl5aceOHY7PNputwmoDAADVj8vDjbu7uwICAkrc32azlao/AAC4vLh8zs2uXbsUFBSkZs2aqW/fvkpLSyu2f3Z2tkJCQhQcHKy77rpLv/76ayVVCgAAqgOXhpvw8HDNmjVL33//vaZOnaqUlBR17dpVJ06cKLJ/y5YtNWPGDH311VeaM2eO8vPz1alTJ+3fv/+ix8jJyVFWVpbTAgAArMtmjDGuLqJARkaGQkJCNHnyZA0aNOiS/c+cOaPWrVsrJiZGEyZMKLJPfHy8xo8fX6g9MzNTXl5ef7lmAABQ8bKysuTt7V2i72+X35Y6n4+Pj0JDQ5WcnFyi/jVr1lT79u2L7R8bG6vMzEzHsm/fvvIqFwAAVEFVKtxkZ2dr9+7dCgwMLFH/vLw8bd26tdj+drtdXl5eTgsAALAul4abMWPGaMWKFUpNTVViYqKio6Pl5uammJgYSVL//v0VGxvr6P/8889r8eLF2rNnjzZt2qR+/fpp7969Gjx4sKtOAQAAVDEufRR8//79iomJUXp6uvz8/NSlSxetXbtWfn5+kqS0tDTVqPG//PXHH39oyJAhOnTokOrXr6/rr79eiYmJatOmjatOAQAAVDFVakJxZSjNhCQAAFA1VNsJxQAAAH8V4QYAAFgK4QYAAFiKy98tVdkKphjxS8UAAFQfBd/bJZkqfNmFm4JXOwQHB7u4EgAAUFonTpyQt7d3sX0uu6el8vPz9fvvv6tevXqy2WyuLsflsrKyFBwcrH379vH0WAVinCsH41x5GOvKwTj/jzFGJ06cUFBQkNPPxBTlsrtyU6NGDV155ZWuLqPK4debKwfjXDkY58rDWFcOxvmcS12xKcCEYgAAYCmEGwAAYCmEm8uc3W5XXFyc7Ha7q0uxNMa5cjDOlYexrhyMc9lcdhOKAQCAtXHlBgAAWArhBgAAWArhBgAAWArhBgAAWArhxuKOHz+uvn37ysvLSz4+Pho0aJCys7OL3eb06dMaPny4fH19VbduXd199906fPhwkX3T09N15ZVXymazKSMjowLOoPqoiLHesmWLYmJiFBwcLE9PT7Vu3VqvvfZaRZ9KlfLmm2+qSZMm8vDwUHh4uNavX19s/88++0ytWrWSh4eH2rZtq4ULFzqtN8boueeeU2BgoDw9PRUREaFdu3ZV5ClUC+U5zmfOnNHYsWPVtm1b1alTR0FBQerfv79+//33ij6NKq+8/z2fb9iwYbLZbJoyZUo5V10NGVhaVFSUadeunVm7dq1ZuXKlueqqq0xMTEyx2wwbNswEBwebH374wWzcuNHcdNNNplOnTkX2veuuu8ztt99uJJk//vijAs6g+qiIsZ4+fboZMWKEWb58udm9e7f54IMPjKenp3njjTcq+nSqhE8++cTUqlXLzJgxw/z6669myJAhxsfHxxw+fLjI/qtXrzZubm7m5ZdfNtu2bTPPPvusqVmzptm6daujz7/+9S/j7e1tvvzyS7NlyxZz5513mqZNm5pTp05V1mlVOeU9zhkZGSYiIsLMnTvXbN++3axZs8bceOON5vrrr6/M06pyKuLfc4H58+ebdu3amaCgIPPvf/+7gs+k6iPcWNi2bduMJLNhwwZH23fffWdsNps5cOBAkdtkZGSYmjVrms8++8zR9ttvvxlJZs2aNU5933rrLXPzzTebH3744bIPNxU91ud75JFHzC233FJ+xVdhN954oxk+fLjjc15engkKCjIJCQlF9r/33ntNz549ndrCw8PNP/7xD2OMMfn5+SYgIMC88sorjvUZGRnGbrebjz/+uALOoHoo73Euyvr1640ks3fv3vIpuhqqqHHev3+/adSokUlKSjIhISGEG2MMt6UsbM2aNfLx8VGHDh0cbREREapRo4bWrVtX5DY//fSTzpw5o4iICEdbq1at1LhxY61Zs8bRtm3bNj3//POaPXv2JV9gdjmoyLG+UGZmpho0aFB+xVdRubm5+umnn5zGp0aNGoqIiLjo+KxZs8apvyRFRkY6+qekpOjQoUNOfby9vRUeHl7smFtZRYxzUTIzM2Wz2eTj41MudVc3FTXO+fn5euCBB/Tkk08qLCysYoqvhvhWsrBDhw6pYcOGTm3u7u5q0KCBDh06dNFtatWqVeh/gPz9/R3b5OTkKCYmRq+88ooaN25cIbVXNxU11hdKTEzU3LlzNXTo0HKpuyo7duyY8vLy5O/v79Re3PgcOnSo2P4Ff5Zmn1ZXEeN8odOnT2vs2LGKiYm5bF/+WFHj/NJLL8nd3V0jRowo/6KrMcJNNfT000/LZrMVu2zfvr3Cjh8bG6vWrVurX79+FXaMqsLVY32+pKQk3XXXXYqLi9Ntt91WKccE/qozZ87o3nvvlTFGU6dOdXU5lvLTTz/ptdde06xZs2Sz2VxdTpXi7uoCUHpPPPGEBg4cWGyfZs2aKSAgQEeOHHFqP3v2rI4fP66AgIAitwsICFBubq4yMjKcrigcPnzYsc2PP/6orVu3at68eZLOPX0iSVdccYWeeeYZjR8/voxnVvW4eqwLbNu2TT169NDQoUP17LPPlulcqpsrrrhCbm5uhZ7UK2p8CgQEBBTbv+DPw4cPKzAw0KnPtddeW47VVx8VMc4FCoLN3r179eOPP162V22kihnnlStX6siRI05X0PPy8vTEE09oypQpSk1NLd+TqE5cPekHFadgkuvGjRsdbYsWLSrRJNd58+Y52rZv3+40yTU5Odls3brVscyYMcNIMomJiRed9W91FTXWxhiTlJRkGjZsaJ588smKO4Eq6sYbbzSPPvqo43NeXp5p1KhRsRMw//a3vzm1dezYsdCE4ldffdWxPjMzkwnF5TzOxhiTm5trevfubcLCwsyRI0cqpvBqprzH+dixY07/W7x161YTFBRkxo4da7Zv315xJ1INEG4sLioqyrRv396sW7fOrFq1yrRo0cLp8eT9+/ebli1bmnXr1jnahg0bZho3bmx+/PFHs3HjRtOxY0fTsWPHix5j2bJll/3TUsZUzFhv3brV+Pn5mX79+pmDBw86lsvly+KTTz4xdrvdzJo1y2zbts0MHTrU+Pj4mEOHDhljjHnggQfM008/7ei/evVq4+7ubl599VXz22+/mbi4uCIfBffx8TFfffWV+eWXX8xdd93Fo+DlPM65ubnmzjvvNFdeeaXZvHmz07/dnJwcl5xjVVAR/54vxNNS5xBuLC49Pd3ExMSYunXrGi8vL/Pggw+aEydOONanpKQYSWbZsmWOtlOnTplHHnnE1K9f39SuXdtER0ebgwcPXvQYhJtzKmKs4+LijKRCS0hISCWemWu98cYbpnHjxqZWrVrmxhtvNGvXrnWsu/nmm82AAQOc+n/66acmNDTU1KpVy4SFhZkFCxY4rc/Pzzfjxo0z/v7+xm63mx49epgdO3ZUxqlUaeU5zgX/1otazv/3fzkq73/PFyLcnGMz5v+fMAEAAGABPC0FAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADuEi3bt00atQoV5dRiM1m05dffunqMvTAAw9o4sSJLjn2rFmzCr2tvbKkpqbKZrNp8+bN5b7v5cuXy2azKSMj45J9t23bpiuvvFJ//vlnudcBVDTCDeAi8+fP14QJExyfmzRpoilTplTa8ePj44t8WeTBgwd1++23V1odRdmyZYsWLlyoESNGuLSOy1mbNm100003afLkya4uBSg1wg3gIg0aNFC9evXKfb+5ubl/afuAgADZ7fZyqqZs3njjDfXp00d169at0OP81bFyBWOMzp49WynHevDBBzV16tRKOx5QXgg3gIucf1uqW7du2rt3r0aPHi2bzSabzebot2rVKnXt2lWenp4KDg7WiBEjnG4VNGnSRBMmTFD//v3l5eWloUOHSpLGjh2r0NBQ1a5dW82aNdO4ceN05swZSeduu4wfP15btmxxHG/WrFmSCt+W2rp1q7p37y5PT0/5+vpq6NChys7OdqwfOHCgevfurVdffVWBgYHy9fXV8OHDHceSpLfeekstWrSQh4eH/P39dc8991x0XPLy8jRv3jz16tXLqb3gPGNiYlSnTh01atRIb775plOfjIwMDR48WH5+fvLy8lL37t21ZcsWx/qCq1XvvfeemjZtKg8Pj+L+irRo0SK1bt1adevWVVRUlA4ePOhYV9Rtxd69e2vgwIFONU+cOFEPPfSQ6tWrp8aNG2vatGlO26xfv17t27eXh4eHOnTooJ9//tlpfcGtpO+++07XX3+97Ha7Vq1apfz8fCUkJKhp06by9PRUu3btNG/ePKdtFy5cqNDQUHl6euqWW25Ramqq0/q9e/eqV69eql+/vurUqaOwsDAtXLjQsf7WW2/V8ePHtWLFimLHCahyXPxuK+CydfPNN5uRI0caY869dPPKK680zz//vOPtycYYk5ycbOrUqWP+/e9/m507d5rVq1eb9u3bm4EDBzr2ExISYry8vMyrr75qkpOTTXJysjHGmAkTJpjVq1eblJQU8/XXXxt/f3/z0ksvGWOMOXnypHniiSdMWFiY43gnT540xhgjyXzxxRfGGGOys7NNYGCg+fvf/262bt1qfvjhB9O0aVOnl/sNGDDAeHl5mWHDhpnffvvNfPPNN6Z27dpm2rRpxhhjNmzYYNzc3MxHH31kUlNTzaZNm8xrr7120XHZtGmTkeR4U/L551mvXj2TkJBgduzYYV5//XXj5uZmFi9e7OgTERFhevXqZTZs2GB27txpnnjiCePr62vS09ONMedeRFqnTh0TFRVlNm3aZLZs2VJkDTNnzjQ1a9Y0ERERZsOGDeann34yrVu3Nvfff3+Rf38F7rrrLqexCQkJMQ0aNDBvvvmm2bVrl0lISDA1atQw27dvN8YYc+LECePn52fuv/9+k5SUZL755hvTrFkzI8n8/PPPxpj/vZj2mmuuMYsXLzbJyckmPT3dvPDCC6ZVq1bm+++/N7t37zYzZ840drvdLF++3BhjTFpamrHb7ebxxx8327dvN3PmzDH+/v5OL7nt2bOnufXWW80vv/xidu/ebb755huzYsUKp3MKDw83cXFxF/37Aqoiwg3gIhd+ORb1Nt9BgwaZoUOHOrWtXLnS1KhRw5w6dcqxXe/evS95vFdeecVcf/31js9xcXGmXbt2hfqdH26mTZtm6tevb7Kzsx3rFyxYYGrUqOEIHwMGDDAhISHm7Nmzjj59+vQx//d//2eMMebzzz83Xl5eJisr65I1GmPMF198Ydzc3Ex+fr5Te0hIiImKinJq+7//+z9z++23G2POjYuXl5c5ffq0U5/mzZubd955x3HONWvWNEeOHCm2hpkzZxpJjqBojDFvvvmm8ff3d3wuabjp16+f43N+fr5p2LChmTp1qjHGmHfeecf4+vo6/i6NMWbq1KlFhpsvv/zS0ef06dOmdu3aJjEx0en4gwYNMjExMcYYY2JjY02bNm2c1o8dO9Yp3LRt29bEx8cXOxbR0dFOYRqoDtxddcUIwKVt2bJFv/zyiz788ENHmzFG+fn5SklJUevWrSVJHTp0KLTt3Llz9frrr2v37t3Kzs7W2bNn5eXlVarj//bbb2rXrp3q1KnjaOvcubPy8/O1Y8cO+fv7S5LCwsLk5ubm6BMYGKitW7dKOndrIyQkRM2aNVNUVJSioqIUHR2t2rVrF3nMU6dOyW63O92aK9CxY8dCnwsmYW/ZskXZ2dny9fUttL/du3c7PoeEhMjPz++S5167dm01b97c6ZyOHDlyye0udM011zj+22azKSAgwLGf3377Tddcc43T7bELz7HA+X/HycnJOnnypG699VanPrm5uWrfvr1j3+Hh4U7rL9z3iBEj9PDDD2vx4sWKiIjQ3Xff7VSvJHl6eurkyZMlPV2gSiDcAFVYdna2/vGPfxT51FDjxo0d/31++JCkNWvWqG/fvho/frwiIyPl7e2tTz75RJMmTaqQOmvWrOn02WazKT8/X5JUr149bdq0ScuXL9fixYv13HPPKT4+Xhs2bCjycesrrrhCJ0+eVG5urmrVqlXiGrKzsxUYGKjly5cXWnf+cS4cq9KckzHG8blGjRpOnyU5zTMqbj8FY1Ma59ddMOdpwYIFatSokVO/0kwGHzx4sCIjI7VgwQItXrxYCQkJmjRpkh577DFHn+PHjzuFPKA6YEIxUEXUqlVLeXl5Tm3XXXedtm3bpquuuqrQUtwXf2JiokJCQvTMM8+oQ4cOatGihfbu3XvJ412odevW2rJli9ME5tWrV6tGjRpq2bJlic/N3d1dERERevnll/XLL78oNTVVP/74Y5F9Cx5P37ZtW6F1a9euLfS54OrVddddp0OHDsnd3b3QWF1xxRUlrrWk/Pz8nCYY5+XlKSkpqVT7aN26tX755RedPn3a0XbhORalTZs2stvtSktLK3SuwcHBjn2vX7/eabui9h0cHKxhw4Zp/vz5euKJJ/Tuu+86rU9KSnJcDQKqC8INUEU0adJE//3vf3XgwAEdO3ZM0rknnhITE/Xoo49q8+bN2rVrl7766is9+uijxe6rRYsWSktL0yeffKLdu3fr9ddf1xdffFHoeCkpKdq8ebOOHTumnJycQvvp27evPDw8NGDAACUlJWnZsmV67LHH9MADDzhuSV3Kt99+q9dff12bN2/W3r17NXv2bOXn5180HPn5+em6667TqlWrCq1bvXq1Xn75Ze3cuVNvvvmmPvvsM40cOVKSFBERoY4dO6p3795avHixUlNTlZiYqGeeeUYbN24sUa2l0b17dy1YsEALFizQ9u3b9fDDD5fox/HOd//998tms2nIkCHatm2bFi5cqFdfffWS29WrV09jxozR6NGj9f7772v37t3atGmT3njjDb3//vuSpGHDhmnXrl168skntWPHDn300UeOJ+IKjBo1SosWLVJKSoo2bdqkZcuWOcKidO4HBQ8cOKCIiIhSnRfgaoQboIp4/vnnlZqaqubNmzvmhFxzzTVasWKFdu7cqa5du6p9+/Z67rnnFBQUVOy+7rzzTo0ePVqPPvqorr32WiUmJmrcuHFOfe6++25FRUXplltukZ+fnz7++ONC+6ldu7YWLVqk48eP64YbbtA999yjHj166D//+U+Jz8vHx0fz589X9+7d1bp1a7399tv6+OOPFRYWdtFtBg8e7DTPqMATTzyhjRs3qn379nrhhRc0efJkRUZGSjp3u2fhwoX6f//v/+nBBx9UaGio7rvvPu3du7fEQaw0HnroIQ0YMED9+/fXzTffrGbNmumWW24p1T7q1q2rb775Rlu3blX79u31zDPP6KWXXirRthMmTNC4ceOUkJCg1q1bKyoqSgsWLFDTpk0lnbtt+fnnn+vLL79Uu3bt9Pbbbxf6xee8vDwNHz7csX1oaKjeeustx/qPP/5Yt912m0JCQkp1XoCr2cyFN40BwMVOnTqlli1bau7cuY5JsE2aNNGoUaOq5CsrrCg3N1ctWrTQRx99pM6dO7u6HKBUuHIDoMrx9PTU7NmzHbfnUPnS0tL0z3/+k2CDaomnpQBUSd26dXN1CZe1ggnKQHXEbSkAAGAp3JYCAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACW8v8BSP+Lq1ob/woAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "print('here we start our logistic regression project \\n')\n",
    "print('Problem Statement: You are given a dataset (\"data.h5\") containing: - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px). You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat. \\n')\n",
    "\n",
    "def load_dataset():\n",
    "  print('loading the dataset. we run a method instead of including a module named load_dataset that is required in the imports in the course module. \\n')\n",
    "  # file_data = scipy.io.loadmat('dataset.mat')\n",
    "  train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "  train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "  train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "  test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "  test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "  test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "  classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "  \n",
    "  print('Each line of your train_set_x_orig and test_set_x_orig is an array representing an image \\n')\n",
    "\n",
    "  train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "  test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "  return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "\n",
    "# check an example of a picture that exists in the dataset loaded\n",
    "print('the following is the image number 7 of the dataset. \\n')\n",
    "index = 7\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "plt.close()\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture. \\n\")\n",
    "\n",
    "print('we loaded the following data: \\n')\n",
    "print(train_set_x_orig.shape)\n",
    "print('\\n')\n",
    "\n",
    "# exercise \n",
    "\n",
    "print('Exercise: Find the values for: - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0]. \\n') \n",
    "\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape) + str('\\n'))\n",
    "\n",
    "# exercise 2\n",
    "\n",
    "print('Exercise: Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px num_px 3, 1). \\n')\n",
    "\n",
    "# A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b*c*d, a) is to use:\n",
    "# X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "# train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "# test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "train_x_product = train_set_x_orig.shape[1] * train_set_x_orig.shape[2] * train_set_x_orig.shape[3]\n",
    "test_x_product = test_set_x_orig.shape[1] * test_set_x_orig.shape[2] * test_set_x_orig.shape[3]\n",
    "\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_x_product, train_set_x_orig.shape[0] )\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_x_product, test_set_x_orig.shape[0])\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print ('sanity check after reshaping: ' + str(train_set_x_flatten[0:5,0]) + '\\n')\n",
    "\n",
    "print('To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255. One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel). \\n')\n",
    "\n",
    "# lets standardize the dataset\n",
    "\n",
    "train_set_x = train_set_x_flatten/255\n",
    "test_set_x = test_set_x_flatten/255\n",
    "\n",
    "# **What you need to remember:**\n",
    "# Common steps for pre-processing a new dataset are:\n",
    "# Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\n",
    "# Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)\n",
    "# \"Standardize\" the data\n",
    "\n",
    "print(\"let's now determine how to create a neural network that help us solve the problem of identifying cat images\\n\")\n",
    "print(\"the following is a set of steps we will use to solve the problem\\n\")\n",
    "print(\"1. initialize the parameters of the model.\\n\")\n",
    "print(\"2. learn the parameters for the model by minimizing the cost function.\\n\")\n",
    "print(\"3. use the learned parameters to make predictions over the test set.\\n\")\n",
    "print(\"4. analyse the results and conclude.\\n\")\n",
    "\n",
    "# building the parts of our algorithm: \n",
    "# 1. define the model structure\n",
    "# 2. initialize the parameters for the model\n",
    "# 3. loop: calculate current loss (forward propagation), calculate current gradient (backward propagation), update the parameters (gradient descent)\n",
    "# build each step separately and calculate a method called model where everything is integrated\n",
    "\n",
    "# exercise 3 \n",
    "print(\"implement sigmoid function to make predictions. use np.exp()\")\n",
    "\n",
    "# GRADED FUNCTION: sigmoid \n",
    "\n",
    "print(\"we will start by defining the model structure and the helper methods to execute calculations base in our activation function, and the initialization step which requires an initialize function.\\n\")\n",
    "print(\"1. define the model structure.\\n\")\n",
    "\n",
    "def sigmoid(z):\n",
    "  \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "  sig = 1/(1 + np.exp(-z))\n",
    "  \n",
    "  return sig\n",
    "\n",
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0, 2]))))\n",
    "print (\"\\nExercise: Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation.\\n\")\n",
    "\n",
    "# GRADED FUNCTION: initialize with zeros\n",
    "\n",
    "print(\"2. initialize the model parameter (attributes).\\n\")\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "  \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "  \n",
    "  w = np.zeros((dim, 1))\n",
    "  b = 0\n",
    "  \n",
    "  assert(w.shape == (dim, 1))\n",
    "  assert(isinstance(b, float) or isinstance(b, int))\n",
    "  \n",
    "  return w, b\n",
    "\n",
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print(\"w = \" + str(w) + \"\\n\")\n",
    "print(\"b = \" + str(b) + \"\\n\")\n",
    "\n",
    "print(\"3. calculate current loss, current gradient and update the parameters.\\n\")\n",
    "print(\"Exercise: implement a function => propagate() that computes the cost function and its gradients.\\n\")\n",
    "\n",
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "# forward propagation\n",
    "# - you get X (dataset)\n",
    "# - you compute the activation function A = sigmoid(wT.X + b) = (a(1), a(2), a(3)...a(m))\n",
    "# - you calculate the cost function J = -1/m . summation(1 to m) (y(i). log(a(i)) + (1 - y(i). log(1 - a(i))))\n",
    "\n",
    "# formulas to use:\n",
    "# dj/dw = 1/m . (X(A - Y)T) => a minus y transpose\n",
    "# dj/db = 1/m . summation(1 to m) (a(i) -y(i))\n",
    "\n",
    "def propagate(weights, bias, data, labels):\n",
    "  \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w(weights) -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b(bias) -- bias, a scalar\n",
    "    X(data) -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y(labels) -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "  data_shape = data.shape[1]\n",
    "  \n",
    "  # FORWARD PROPAGATION (FROM data TO cost)\n",
    "  activations = sigmoid(np.dot(weights.T, data) + bias)\n",
    "  cost = np.sum((- np.log(activations) * labels + (- np.log(1 - activations)) * (1 - labels)))/ data_shape\n",
    "  # END FORWARD PROPAGATION  \n",
    "  \n",
    "  # BACKWARD PROPAGATION (TO FIND gradients)\n",
    "  dw = (np.dot(data, (activations-labels).T))/data_shape\n",
    "  db = (np.sum(activations-labels))/data_shape\n",
    "  # END BACKWARD PROPAGATIONS\n",
    "  \n",
    "  assert(dw.shape == weights.shape)\n",
    "  assert(db.dtype == float)\n",
    "  cost = np.squeeze(cost)\n",
    "  assert(cost.shape == ())\n",
    "  \n",
    "  gradients = {\"dw\": dw, \n",
    "               \"db\": db}\n",
    "  \n",
    "  return gradients, cost\n",
    "\n",
    "weights, bias, data, labels = (np.array([[1.], [2.]]), \n",
    "                               2., \n",
    "                               np.array([[1., 2., -1.],[3., 4., -3.2]]), \n",
    "                               np.array([[1, 0, 1]]))\n",
    "grads, cost = propagate(weights, bias, data, labels)\n",
    "print (\"calculate the propagation function with weights = 1. and 2. , bias = 2. , data = [1.,2.,-1.],[3.,4.,-3.2] and  labels = [1, 0, 1]. \\n\")\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))\n",
    "\n",
    "# optimization\n",
    "\n",
    "print(\"Exercise: Implement the optimization function in the cell below.\\n\")\n",
    "print(\"Write down the optimization function. The goal is to learn weights and bias by minimizing the cost function J. For a parameter theta, the update rule is theta = theta - activation * dtheta, where activation is the learning rate.\\n\")\n",
    "\n",
    "def optimize(weights, bias, data, labels, num_iterations, learning_rate, print_cost = False):\n",
    "  \"\"\"\n",
    "    This function optimizes weights and bias by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    weights -- a numpy array of size (num_px * num_px * 3, 1)\n",
    "    bias -- a scalar\n",
    "    data -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    labels -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "  \n",
    "  costs = []\n",
    "  \n",
    "  for iteration in range(num_iterations):\n",
    "    # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "    ## START CODE HERE ##\n",
    "    grads, cost = propagate(weights, bias, data, labels)\n",
    "    ## END CODE HERE ##\n",
    "    \n",
    "    # Retrieve derivatives from grads\n",
    "    dw = grads[\"dw\"]\n",
    "    db = grads[\"db\"]\n",
    "    \n",
    "    # update rule (≈ 2 lines of code)\n",
    "    ## START CODE HERE ##\n",
    "    weights = weights - learning_rate * dw\n",
    "    bias = bias - learning_rate * db\n",
    "    ## END CODE HERE ##\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "      costs.append(cost)\n",
    "    \n",
    "    # Record the costs\n",
    "    if print_cost and iteration % 100 == 0:\n",
    "      costs.append(cost)\n",
    "      print(\"Cost after iteration %i: %f\" %(iteration, cost))\n",
    "          \n",
    "          \n",
    "  params = {\n",
    "    \"weights\": weights,\n",
    "    \"bias\": bias\n",
    "  }\n",
    "  \n",
    "  grads = {\n",
    "    \"dw\": dw,\n",
    "    \"db\": db\n",
    "  }\n",
    "  \n",
    "  return params, grads, costs\n",
    "\n",
    "# evaluate the optimization of the model and plot the costs obtained from the evaluation of the cost function\n",
    "\n",
    "params, grads, costs = optimize(weights, bias, data, labels, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"weights = \" + str(params[\"weights\"]))\n",
    "print (\"bias = \" + str(params[\"bias\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "\n",
    "# plot the costs\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(0.009))\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T15:52:00.024231884Z",
     "start_time": "2024-02-16T15:51:59.789655171Z"
    }
   },
   "id": "65e350bd9a7f69ff",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8e5fe2afc440d098"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
